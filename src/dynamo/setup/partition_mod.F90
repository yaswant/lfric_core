!-------------------------------------------------------------------------------
! (c) The copyright relating to this work is owned jointly by the Crown, 
! Met Office and NERC 2014. 
! However, it has been created with the help of the GungHo Consortium, 
! whose members are identified at https://puma.nerc.ac.uk/trac/GungHo/wiki
!-------------------------------------------------------------------------------

!> @brief A module providing a partitioning class

!> @details When instantiated, this module partitions the mesh for the
!> supported mesh types: bi-periodic plane and cubed sphere.
!> It provides a list of cells known to this partition. The order of the
!> list is as follows:
!> The cells that are wholly owned by the partition are followed by the cells
!> that might have dofs in the halo and finally, the cells that form the
!> the halo. The first depth of halos are generated by applying a stencil
!> to the owned cells. Subsequent depths of halo are generated by applying
!> the stencil to the previous depth of halo cells

module partition_mod

use global_mesh_mod, only : global_mesh_type

use ESMF

implicit none

private

public :: partitioner_cubedsphere, &
          partitioner_biperiodic, &
          partitioner_cubedsphere_serial, &
          partitioner_interface

type, public :: partition_type
  private
!> The number of the MPI rank
  integer              :: local_rank
!> Total number of MPI ranks in this execution
  integer              :: total_ranks
!> A List of global cell ids known to this partition ordered with core cells
!> first followed by the owned cells and finally the halo cells ordered by
!> depth of halo
  integer, allocatable :: global_cell_id( : )
!> A list of the ranks that own all the cells known to this partition
!> held in the order of cells in the <code>global_cell_id</code> array
  integer, pointer     :: cell_owner( : )
!> The number of "core" cells in the <code>global_cell_id</code> list
  integer              :: num_core
!> The number of "owned" cells in the <code>global_cell_id</code> list
  integer              :: num_owned
!> The number of "halo" cells in the <code>global_cell_id</code> list - 
!> one entry for each depth of halo
  integer, allocatable :: num_halo( : )
!> The depth to which halos are generated
  integer              :: halo_depth
!> The total number of cells in the global domain
  integer              :: global_num_cells
contains
!> Returns the total of core, owned and all halo cells in a 2d slice on the
!> local partition
!> @return num_cells The total number of the core, owned and all halo cells
!> on the local partition
  procedure, public :: get_num_cells_in_layer
!> Returns the total number of core cells in a 2d slice on the local partition
!> @return core_cells The total number of core cells on the local partition
  procedure, public :: get_num_cells_core
!> Returns the total number of owned cells in a 2d slice on the local partition
!> @return core_cells The total number of owned cells on the local partition
  procedure, public :: get_num_cells_owned
!> Returns the total number of halo cells in a particular depth of halo in a 2d
!> slice on the local partition
!> @param[in] depth The depth of the halo being quried
!> @return core_cells The total number of halo cells of the particular depth 
!> on the local partition
  procedure, public :: get_num_cells_halo
!> Returns the owner of a cell on the local partition
!> @param[in] cell_number The local id of of the cell being queried 
!> @return cell_owner The owner of the given cell
  procedure, public :: get_cell_owner
!> Returns the global index of the cell that corresponds to the given
!! local index on the local partition
!> @param[in] lid The id of a cell in local index space
!> @return gid The id of a cel in global index space
  procedure, public :: get_gid_from_lid
!> @brief Returns the local index of the cell on the local
!> partition that corresponds to the given global index.
!> @param[in] gid The global index to search for on the local partition
!> @return lid The local index that corresponds to the given global index
!> or -1 if the cell with the given global index is not present of the local partition
  procedure, public :: get_lid_from_gid
end type partition_type

interface partition_type
  module procedure partition_constructor
end interface

interface
  subroutine partitioner_interface( global_mesh, &
                                    xproc, &
                                    yproc, &
                                    local_rank, &
                                    total_ranks, &
                                    halo_depth, &
                                    global_cell_id, &
                                    num_core, &
                                    num_owned, &
                                    num_halo )
    import :: global_mesh_type

    type(global_mesh_type), intent(in) :: global_mesh
    integer,                intent(in)    :: xproc, yproc, local_rank, total_ranks
    integer, allocatable,   intent(inout) :: global_cell_id( : )
    integer,                intent(in)    :: halo_depth
    integer,                intent(out)   ::  num_core, num_owned, num_halo( : )
  end subroutine partitioner_interface
end interface 

contains 

!> Construct a <code>partition_type</code> object 
!>
!> @param [in] global_mesh A global mesh object that describes the layout
!>                         of the global mesh
!> @param [in] partitioner A function pointer to the function that will perform
!>                         the partitioning 
!> @param [in] xproc Number of ranks to partition the mesh over in the
!>                   x-direction (across a single face  for a cubed-sphere mesh)
!> @param [in] yproc Number of ranks to partition the mesh over in the
!>                   y-direction (across a single face  for a cubed-sphere mesh)
!> @param [in] halo_depth The depth to which halos will be created
!> @param [in] local_rank Number of the local process rank 
!> @param [in] total_ranks Total number of process ranks available
!> @return self the partition object
!>
function partition_constructor( global_mesh, &
                                partitioner, &
                                xproc, &
                                yproc, &
                                halo_depth, &
                                local_rank, &
                                total_ranks) result(self)

implicit none

type(global_mesh_type), intent(in) :: global_mesh

procedure(partitioner_interface) :: partitioner

integer, intent(in) :: xproc
integer, intent(in) :: yproc
integer, intent(in) :: halo_depth
integer, intent(in) :: local_rank
integer, intent(in) :: total_ranks

type(partition_type) :: self

type(ESMF_DistGrid) :: distgrid
integer :: rc
type(ESMF_Array) :: temporary_esmf_array
type(ESMF_RouteHandle) :: haloHandle
integer :: cell

self%local_rank = local_rank
self%total_ranks = total_ranks
self%halo_depth = halo_depth
allocate( self%num_halo(halo_depth) )
self%global_num_cells = global_mesh%get_ncells()

! Call the partitioner that has been passed into the routine as a procedure pointer
call partitioner( global_mesh, &
                  xproc, yproc, &
                  local_rank, &
                  total_ranks, &
                  halo_depth, &
                  self%global_cell_id, &
                  self%num_core, &
                  self%num_owned, &
                  self%num_halo )

! Calculate ownership of cells known to the local partition
! by filling the locally owned cells with the local rank and performing
! a halo-swap to fill in the owners of all the halo cells.
!
! Set up the ESMF structures required to perform a halo swap 
!
! Create an ESMF DistGrid, which describes which partition owns which cells
distgrid = ESMF_DistGridCreate( arbSeqIndexList=self%global_cell_id(1:self%num_core+self%num_owned), &
                                rc=rc )
if (rc /= ESMF_SUCCESS) call ESMF_Finalize( endflag=ESMF_END_ABORT )

! Can only halo-swap an ESMF array so set one up that's big enough to hold all
! the owned cells and all the halos
temporary_esmf_array = ESMF_ArrayCreate( distgrid=distgrid, &
                                         typekind=ESMF_TYPEKIND_I4, &
                                         haloSeqIndexList=self%global_cell_id(self%num_core+self%num_owned+1 &
                                                                              :self%get_num_cells_in_layer()), &
                                         rc=rc )
if (rc /= ESMF_SUCCESS) call ESMF_Finalize( endflag=ESMF_END_ABORT )

! Point our Fortran array at the space we've set up in the ESMF array
call ESMF_ArrayGet( array=temporary_esmf_array, &
                    farrayPtr=self%cell_owner, &
                    rc=rc )
if (rc /= ESMF_SUCCESS) call ESMF_Finalize( endflag=ESMF_END_ABORT )

! Calculate the routing table required to perform the halo-swap, so the
! code knows where to find the values it needs to fill in the halos
call ESMF_ArrayHaloStore( array=temporary_esmf_array, &
                          routehandle=haloHandle, &
                          rc=rc )
if (rc /= ESMF_SUCCESS) call ESMF_Finalize( endflag=ESMF_END_ABORT )

! Set ownership of all core + owned cells to the local rank id - halo cells are unset
do cell = 1,self%num_core+self%num_owned
  self%cell_owner(cell)=local_rank
end do

! Do the halo swap to fill in the halo cell ownership
call ESMF_ArrayHalo( temporary_esmf_array, &
                     routehandle=haloHandle, &
                     rc=rc )
if (rc /= ESMF_SUCCESS) call ESMF_Finalize(endflag=ESMF_END_ABORT)

end function partition_constructor


!> Partitions the mesh on a bi-periodic plane. It returns the global ids of
!> the cells in the given partition.
!>
!> @param [in] global_mesh A global mesh object that describes the layout
!>                         of the global mesh
!> @param [in] xproc Number of processors along x-direction
!> @param [in] yproc Number of processors along y-direction
!> @param local_rank [in] Local MPI rank number
!> @param total_ranks [in] Total number of MPI ranks
!> @param [in] halo_depth The depth to which halos will be created
!> @param partitioned_cells [out] Returned array that holds the global ids of all
!>                          cells in local partition
!> @param num_core [out] Number of cells that are wholly owned by the partition 
!>                 (i.e. all dofs in these cells are wholly owned by the
!>                 partition). The cell ids of these cells appear first in the
!>                 <code>partitioned_cells</code> array
!> @param num_owned [out] Number of cells that are owned by the partition,
!>                  but may have dofs that are also owned by halo cells. The
!>                  cell ids of these cells follow the core cells in the
!>                  <code>partitioned_cells</code> array
!> @param num_halo [out] Number of cells that are halo cells. The cell ids 
!>                 of these cells follow the owned cells in the
!>                 <code>partitioned_cells</code> array
  subroutine partitioner_biperiodic( global_mesh, &
                                     xproc, &
                                     yproc, &
                                     local_rank, &
                                     total_ranks, &
                                     halo_depth, &
                                     partitioned_cells, &
                                     num_core, &
                                     num_owned, &
                                     num_halo )

! The cells in the biperiodic domain are numbered (globally) starting at 1
! in the SW corner and incrementing to num_cells_x in the SE corner. This
! repeats over the next row (to the north) and repeats until cell
! (num_cells_x*num_cells_y) is reached in the NE corner

  use linked_list_mod, only : linked_list_type, add_item, add_unique_item, clear_list
  use log_mod,         only : log_event, LOG_LEVEL_ERROR

  implicit none

  type(global_mesh_type), intent(in) :: global_mesh

  integer,               intent(in)    :: xproc
  integer,               intent(in)    :: yproc
  integer,               intent(in)    :: local_rank
  integer,               intent(in)    :: total_ranks
  integer,               intent(in)    :: halo_depth
  integer, allocatable,  intent(inout) :: partitioned_cells( : )
  integer,               intent(out)   :: num_core
  integer,               intent(out)   :: num_owned
  integer,               intent(out)   :: num_halo( : )

  integer :: local_xproc, local_yproc ! x- and y-dirn processor id of this partition
  integer :: start_x   ! global cell id of start of the domain on this partition in x-dirn
  integer :: num_x     ! number of cells in the domain on this partition in x-dirn
  integer :: start_y   ! global cell id of start of the domain on this partition in y-dirn
  integer :: num_y     ! number of cells in the domain on this partition in y-dirn
  integer :: num_in_list !total number of cells known to this partition
  integer :: num_added ! number of cells added to the linked list 
  integer :: ix, iy    ! loop counters over cells on this partition in x- and y-dirns

  type(linked_list_type), pointer :: curr=>null()  ! the current position at which items will be added
                                                   ! to the list that holds cells known to this partition
  type(linked_list_type), pointer :: start=>null() ! start of the list that holds cells known to this partition
  type(linked_list_type), pointer :: last_core=>null() ! location of the last core cell in the list of cells
  type(linked_list_type), pointer :: last_owned=>null() ! location of the last owned cell in the list of cells
  type(linked_list_type), pointer :: last_halo=>null() ! location of the last halo cell in the list of cells
  type(linked_list_type), pointer :: curr_pos=>null() ! current position when looping over subsections of cells

  integer :: i         ! loop counter over all items being sorted in the bubble sort
  integer :: swap_temp ! temporary swap space used to swap items in the bubble sort
  logical :: swapped   ! flag set to true if the current iteration of the bubble sort swapped any items
  integer :: start_sort, end_sort ! range over which to sort cells
  integer :: depth     ! counter over the halo depths
  integer :: orig_num_in_list ! number of cells in list before halos are added

  if( xproc*yproc /= total_ranks )then
   call log_event( 'Invalid decomposition used for biperiodic partitioner', &
                   LOG_LEVEL_ERROR )
  endif

  !convert the local rank number into a local xproc and yproc
  local_xproc = modulo(local_rank,xproc)
  local_yproc = local_rank/xproc

  !Work out the start index and number of cells (in x- and y-dirn) for 
  !the local partition - this algorithm should spread out the number of
  !cells each partition gets fairly evenly
  start_x = int( ( real( local_xproc ) * real( global_mesh%get_num_cells_x() )/ &
                   real( xproc ) ) + 0.5 ) + 1
  num_x   = int( ( real( local_xproc+1 ) * real( global_mesh%get_num_cells_x() )/ &
                   real( xproc ) ) + 0.5 ) - start_x + 1

  start_y = int( ( real( local_yproc ) * real( global_mesh%get_num_cells_y() )/ &
                   real( yproc ) ) + 0.5 ) + 1
  num_y   = int( ( real( local_yproc + 1 ) * real( global_mesh%get_num_cells_y() )/ &
                   real( yproc ) ) + 0.5 ) - start_y + 1

  !Create a linked list of cells known to this partition
  !Start with the core cells - those with all dofs wholly owned by the partition
  num_in_list = 0
  do iy = start_y + 1, start_y+num_y - 2
    do ix = start_x + 1, start_x+num_x - 2
      call add_item( curr,global_mesh%get_cell_id(1, ix-1, iy-1) )
      if(.not.associated(start))start => curr
      num_in_list = num_in_list+1
    end do
  end do
  num_core = num_in_list

  ! Store location of last core cell in the linked list 
  last_core => curr

  ! Now add the owned cells - those still owned by the partition -
  ! but may have dofs shared with halo cells
  ! Those cells along the top/bottom
  do ix = start_x, start_x+num_x-1
    call add_item( curr,global_mesh%get_cell_id(1, ix-1, start_y-1) )
    num_in_list = num_in_list+1
    if(.not.associated(start))start => curr
    call add_unique_item( start,curr,global_mesh%get_cell_id(1, ix-1, start_y+num_y-2), num_added )
    num_in_list = num_in_list+num_added
  end do
  ! Those along the left/right
  do iy = start_y+1, start_y+num_y-2
    call add_unique_item( start,curr,global_mesh%get_cell_id(1, start_x-1, iy-1), num_added )
    num_in_list = num_in_list+num_added
    call add_unique_item( start,curr,global_mesh%get_cell_id(1, start_x+num_x-2, iy-1), num_added )
    num_in_list = num_in_list+num_added
  end do
  num_owned = num_in_list-num_core

  ! Store location of last owned cell in the linked list 
  last_owned => curr

  ! Add all cells that are in a single depth halo around each of the owned cells
  ! Start by applying a stencil around the first cell after the last core cell
  ! (i.e. the first owned cell) - or the first cell (if there are no core cells)
  if( associated(last_core) )then
   curr_pos => last_core%next
  else
   curr_pos => start
  end if
  orig_num_in_list = num_in_list
  call apply_stencil( global_mesh, &
                      curr_pos, &
                      num_owned, &
                      start, &
                      curr, &
                      num_in_list )

  num_halo(1) = num_in_list - orig_num_in_list

  curr_pos => last_owned%next
  do depth = 2,halo_depth
    ! Store location of last halo cell in the linked list 
    last_halo => curr
    orig_num_in_list = num_in_list
    call apply_stencil( global_mesh, &
                        curr_pos, &
                        num_halo(depth-1), &
                        start, &
                        curr, &
                        num_in_list )
    num_halo(depth) = num_in_list - orig_num_in_list
    curr_pos = last_halo%next
  end do

  allocate( partitioned_cells(num_in_list) )
  curr => start
  do i = 1,num_in_list
    partitioned_cells(i) = curr%dat
    curr => curr%next
  end do

  !Deallocate the list
  call clear_list( start )

  ! Cell ids within the separate groups have to be in numerical order.
  ! Core cells are already correctly ordered so now (bubble) sort the other groups
  !
  !Sort owned cells
  start_sort = num_core + 1
  end_sort = num_core + num_owned
  do
    swapped = .false.
    do i = start_sort,end_sort-1
      if(partitioned_cells(i) > partitioned_cells(i+1))then
        swap_temp = partitioned_cells(i)
        partitioned_cells(i) = partitioned_cells(i+1)
        partitioned_cells(i+1) = swap_temp
        swapped = .true.
      end if
    end do
    if( .not.swapped )exit
  end do
  !
  !Sort halo cells in their groups of halo depths
  do depth = 1,halo_depth
    start_sort = end_sort + 1
    end_sort = start_sort + num_halo(depth) - 1
    do
      swapped = .false.
      do i = start_sort,end_sort-1
        if(partitioned_cells(i) > partitioned_cells(i+1))then
          swap_temp = partitioned_cells(i)
          partitioned_cells(i) = partitioned_cells(i+1)
          partitioned_cells(i+1) = swap_temp
          swapped = .true.
        end if
      end do
      if( .not.swapped )exit
    end do
  end do

  end subroutine partitioner_biperiodic


!> Returns a single partition of cubed-sphere mesh for use when running the 
!> code in serial
!>
!> @param [in] global_mesh A global mesh object that describes the layout
!>                         of the global mesh
!> @param [in] xproc Number of processors along x-direction
!> @param [in] yproc Number of processors along y-direction
!> @param local_rank [in] Local MPI rank number
!> @param total_ranks [in] Total number of MPI ranks
!> @param [in] halo_depth The depth to which halos will be created
!> @param partitioned_cells [out] Returned array that holds the global ids of all
!>                          cells in local partition
!> @param num_core [out] Number of cells that are wholly owned by the partition 
!>                 (i.e. all dofs in these cells are wholly owned by the
!>                 partition). The cell ids of these cells appear first in the
!>                 <code>partitioned_cells</code> array
!> @param num_owned [out] Number of cells that are owned by the partition,
!>                  but may have dofs that are also owned by halo cells. The
!>                  cell ids of these cells follow the core cells in the
!>                  <code>partitioned_cells</code> array
!> @param num_halo [out] Number of cells that are halo cells. The cell ids 
!>                 of these cells follow the owned cells in the
!>                 <code>partitioned_cells</code> array
  subroutine partitioner_cubedsphere_serial( global_mesh, &
                                             xproc, &
                                             yproc, &
                                             local_rank, &
                                             total_ranks, &
                                             halo_depth, &
                                             partitioned_cells, &
                                             num_core, &
                                             num_owned, &
                                             num_halo )

  use log_mod, only : log_event, LOG_LEVEL_ERROR

! The general partitioner for a cubed-sphere mesh returns a minimum of one
! partition per cubed-sphere "face". In order to run the code serially
! a special case for creating a single partition with all points is required.
! So here. we just return one big partition that holds everything
  implicit none

  type(global_mesh_type), intent(in) :: global_mesh

  integer,              intent(in)    :: xproc
  integer,              intent(in)    :: yproc
  integer,              intent(in)    :: local_rank
  integer,              intent(in)    :: total_ranks
  integer,              intent(in)    :: halo_depth
  integer, allocatable, intent(inout) :: partitioned_cells( : )
  integer,              intent(out)   :: num_core
  integer,              intent(out)   :: num_owned
  integer,              intent(out)   :: num_halo( : )

  integer :: i
  integer :: depth ! loop counter over halo depths

  if( total_ranks /= 1 .or. local_rank /= 0 )then
   call log_event( 'Can only use the serial partitioner with a single process',&
     LOG_LEVEL_ERROR )
  endif

  if( xproc /= 1 .or. yproc /= 1)then
   call log_event( 'Invalid decomposition used for serial partitioner',&
     LOG_LEVEL_ERROR )
  endif

  num_core = 6 * global_mesh%get_num_cells_x() * &
                 global_mesh%get_num_cells_y()
  num_owned = 0
  do depth = 1, halo_depth
    num_halo(depth) = 0
  end do

  allocate(partitioned_cells(num_core))
  do i = 1,num_core
    partitioned_cells(i)= i
  end do

  end subroutine partitioner_cubedsphere_serial


!> Partitions the mesh on a cubed sphere. It returns the global ids of
!> the cells in the given partition.
!>
!> @param [in] global_mesh A global mesh object that describes the layout
!>                         of the global mesh
!> @param [in] xproc Number of processors along x-direction
!> @param [in] yproc Number of processors along y-direction
!> @param local_rank [in] Local MPI rank number
!> @param total_ranks [in] Total number of MPI ranks
!> @param [in] halo_depth The depth to which halos will be created
!> @param partitioned_cells [out] Returned array that holds the global ids of all
!>                          cells in local partition
!> @param num_core [out] Number of cells that are wholly owned by the partition 
!>                 (i.e. all dofs in these cells are wholly owned by the
!>                 partition). The cell ids of these cells appear first in the
!>                 <code>partitioned_cells</code> array
!> @param num_owned [out] Number of cells that are owned by the partition,
!>                  but may have dofs that are also owned by halo cells. The
!>                  cell ids of these cells follow the core cells in the
!>                  <code>partitioned_cells</code> array
!> @param num_halo [out] Number of cells that are halo cells. The cell ids 
!>                 of these cells follow the owned cells in the
!>                 <code>partitioned_cells</code> array
  subroutine partitioner_cubedsphere( global_mesh, &
                                      xproc, &
                                      yproc, &
                                      local_rank, &
                                      total_ranks, &
                                      halo_depth, &
                                      partitioned_cells, &
                                      num_core, &
                                      num_owned, &
                                      num_halo )

! The cells in the biperiodic domain are numbered (globally) starting at 1
! in the SW corner and incrementing to num_cells_x in the SE corner. This
! repeats over the next row (to the north) and repeats until cell
! (num_cells_x*num_cells_y) is reached in the NE corner

  use linked_list_mod, only : linked_list_type, add_item, add_unique_item, clear_list

  implicit none

  type(global_mesh_type), intent(in) :: global_mesh

  integer,               intent(in)    :: xproc
  integer,               intent(in)    :: yproc
  integer,               intent(in)    :: local_rank
  integer,               intent(in)    :: total_ranks
  integer,               intent(in)    :: halo_depth
  integer, allocatable,  intent(inout) :: partitioned_cells( : )
  integer,               intent(out)   :: num_core
  integer,               intent(out)   :: num_owned
  integer,               intent(out)   :: num_halo( : )

  integer :: face      ! which face of the cube is implied by local_rank (0->5)
  integer :: start_cell ! lowest cell id of the face implaced by local_rank
  integer :: start_rank ! The number of the first rank on the face implied by local_rank
  integer :: local_xproc, local_yproc ! x- and y-dirn processor id of this partition
  integer :: start_x   ! global cell id of start of the domain on this partition in x-dirn
  integer :: num_x     ! number of cells in the domain on this partition in x-dirn
  integer :: start_y   ! global cell id of start of the domain on this partition in y-dirn
  integer :: num_y     ! number of cells in the domain on this partition in y-dirn
  integer :: num_in_list !total number of cells known to this partition
  integer :: num_added ! number of cells added to the linked list 
  integer :: ix, iy    ! loop counters over cells on this partition in x- and y-dirns

  type(linked_list_type), pointer :: curr=>null()  ! the current position at which items will be added
                                                   ! to the list that holds cells known to this partition
  type(linked_list_type), pointer :: start=>null() ! start of the list that holds cells known to this partition
  type(linked_list_type), pointer :: last_core=>null() ! location of the last core cell in the list of cells
  type(linked_list_type), pointer :: last_owned=>null() ! location of the last owned cell in the list of cells
  type(linked_list_type), pointer :: last_halo=>null() ! location of the last halo cell in the list of cells
  type(linked_list_type), pointer :: curr_pos=>null() ! current position when looping over subsections of cells

  integer :: i         ! loop counter over all items being sorted in the bubble sort
  integer :: swap_temp ! temporary swap space used to swap items in the bubble sort
  logical :: swapped   ! flag set to true if the current iteration of the bubble sort swapped any items
  integer :: start_sort, end_sort ! range over which to sort cells
  integer :: depth     ! counter over the halo depths
  integer :: orig_num_in_list ! number of cells in list before halos are added

  !convert the local rank number into a face number and a local xproc and yproc
  face = int(6.0*(real(local_rank)/real(total_ranks)))+1
  start_cell = (global_mesh%get_num_cells_x()*global_mesh%get_num_cells_y())*(face-1)+1
  start_rank = xproc*yproc*(face-1)
  local_xproc = modulo(local_rank-start_rank,xproc)
  local_yproc = (local_rank-start_rank)/xproc

  !Work out the start index and number of cells (in x- and y-dirn) for 
  !the local partition - this algorithm should spread out the number of
  !cells each partition gets fairly evenly
  start_x = int( ( real( local_xproc ) * real( global_mesh%get_num_cells_x() )/ &
                   real( xproc ) ) + 0.5 ) + 1
  num_x   = int( ( real( local_xproc + 1 ) * real( global_mesh%get_num_cells_x() )/ &
                   real( xproc ) ) + 0.5 ) - start_x + 1

  start_y = int( ( real( local_yproc ) * real( global_mesh%get_num_cells_y() )/ &
                   real( yproc ) ) + 0.5 ) + 1
  num_y   = int( ( real( local_yproc + 1 ) * real( global_mesh%get_num_cells_y() )/ &
                   real( yproc ) ) + 0.5 ) - start_y + 1

  !Create a linked list of cells known to this partition
  !Start with the core cells - those with all dofs wholly owned by the partition
  num_in_list = 0
  do iy = start_y + 1, start_y+num_y - 2
    do ix = start_x + 1, start_x+num_x - 2
      call add_item( curr,global_mesh%get_cell_id(start_cell, ix-1, iy-1) )
      if(.not.associated(start))start => curr
      num_in_list = num_in_list + 1
    end do
  end do 
  num_core = num_in_list

  ! Store location of last core cell in the linked list 
  last_core => curr

  ! Now add the owned cells - those still owned by the partition -
  ! but may have dofs shared with halo cells
  ! Those cells along the top/bottom
  do ix = start_x, start_x+num_x-1
    call add_item( curr,global_mesh%get_cell_id(start_cell, ix-1, start_y-1) )
    num_in_list = num_in_list+1
    if(.not.associated(start))start => curr
    call add_unique_item( start,curr,global_mesh%get_cell_id(start_cell, ix-1, start_y+num_y-2), num_added )
    num_in_list = num_in_list+num_added
  end do
  ! Those along the left/right
  do iy = start_y+1, start_y+num_y-2
    call add_unique_item( start,curr,global_mesh%get_cell_id(start_cell, start_x-1, iy-1), num_added )
    num_in_list = num_in_list+num_added
    call add_unique_item( start,curr,global_mesh%get_cell_id(start_cell, start_x+num_x-2, iy-1), num_added )
    num_in_list = num_in_list+num_added
  end do
  num_owned = num_in_list-num_core

  ! Store location of last owned cell in the linked list 
  last_owned => curr

  ! Add all cells that are in a single depth halo around each of the owned cells
  ! Start by applying a stencil around the first cell after the last core cell
  ! (i.e. the first owned cell) - or the first cell (if there are no core cells)
  if(associated(last_core))then
   curr_pos => last_core%next
  else
   curr_pos => start
  end if
  orig_num_in_list = num_in_list
  call apply_stencil( global_mesh, &
                      curr_pos, &
                      num_owned, &
                      start, &
                      curr, &
                      num_in_list )

  num_halo(1) = num_in_list - orig_num_in_list

  curr_pos => last_owned%next
  do depth = 2,halo_depth
    ! Store location of last halo cell in the linked list 
    last_halo => curr
    orig_num_in_list = num_in_list
    call apply_stencil( global_mesh, &
                        curr_pos, &
                        num_halo(depth-1), &
                        start, &
                        curr, &
                        num_in_list )
    num_halo(depth) = num_in_list - orig_num_in_list
    curr_pos = last_halo%next
  end do

  allocate(partitioned_cells(num_in_list))
  curr => start
  do i = 1,num_in_list
    partitioned_cells(i) = curr%dat
    curr => curr%next
  end do

  !Deallocate the list
  call clear_list( start )

  ! Cell ids within the separate groups have to be in numerical order.
  ! Core cells are already correctly ordered so now (bubble) sort the other groups
  !
  !Sort owned cells
  start_sort = num_core + 1
  end_sort = num_core + num_owned
  do
    swapped = .false.
    do i = start_sort,end_sort-1
      if(partitioned_cells(i) > partitioned_cells(i+1))then
        swap_temp = partitioned_cells(i)
        partitioned_cells(i) = partitioned_cells(i+1)
        partitioned_cells(i+1) = swap_temp
        swapped = .true.
      end if
    end do
    if( .not.swapped )exit
  end do
  !
  !Sort halo cells in their groups of halo depths
  do depth = 1,halo_depth
    start_sort = end_sort + 1
    end_sort = start_sort + num_halo(depth) - 1
    do
      swapped = .false.
      do i = start_sort,end_sort-1
        if(partitioned_cells(i) > partitioned_cells(i+1))then
          swap_temp = partitioned_cells(i)
          partitioned_cells(i) = partitioned_cells(i+1)
          partitioned_cells(i+1) = swap_temp
          swapped = .true.
        end if
      end do
      if( .not.swapped )exit
    end do
  end do

  end subroutine partitioner_cubedsphere


!> @brief Applies a single depth stencil around a collection of cells and adds
!> the global ids of the stencil cells to a list of cells known to the partition
!> - if they are not already in the list.
!> @param[in] global_mesh A global mesh object that describes the layout of the global mesh
!> @param[in] input_cells A pointer to the start of a portion of the linked list over which the stencil will be applied
!> @param[in] number_of_cells The number of cells in the portion of the linked list over which the stencil will be applied
!> @param[in] start Start of the linked-list that holds cells known to this partition
!> @param[inout] curr Current position at which items will be added to the list that holds cells known to this partition
!> @param[inout] num_in_list Total number of cells known to this partition
  subroutine apply_stencil( global_mesh, &
                            input_cells, &
                            number_of_cells, &
                            start, &
                            curr, &
                            num_in_list )
  use linked_list_mod, only : linked_list_type, add_unique_item
  use reference_element_mod, only : nverts_h
  implicit none

  type(global_mesh_type), intent(in)             :: global_mesh
  type(linked_list_type), intent(inout), pointer :: input_cells
  integer,                intent(in)             :: number_of_cells
  type(linked_list_type), intent(inout), pointer :: curr
  type(linked_list_type), intent(in),    pointer :: start
  integer,                intent(inout)          :: num_in_list

  integer              :: i,j,k  ! loop counter
  integer              :: cell_id ! the current cell id that the stencil is being applied to
  integer              :: num_added ! number of cells added to the linked list 
  integer, allocatable :: verts(:)
  integer, allocatable :: cells(:)

  allocate( cells( global_mesh%get_max_cells_per_vertex() ) )
  allocate( verts(nverts_h) )

  do i = 1,number_of_cells
    cell_id = input_cells%dat
    call global_mesh%get_vert_on_cell( cell_id, verts )
    do j = 1,nverts_h
      call global_mesh%get_cell_on_vert( verts(j), cells )
      do k = 1,global_mesh%get_max_cells_per_vertex()
        if(cells(k) > 0)then
          call add_unique_item( start,curr,cells(k), num_added )
          num_in_list = num_in_list + num_added
        end if
      end do
    end do
    input_cells => input_cells%next
  end do

  deallocate(verts)
  deallocate(cells)

  end subroutine apply_stencil

function get_cell_owner( self, cell_number ) result ( cell_owner )

  implicit none

  class(partition_type), intent(in) :: self

  integer, intent(in) :: cell_number

  integer :: cell_owner

  cell_owner=self%cell_owner(cell_number)

end function get_cell_owner


function get_num_cells_in_layer( self ) result ( num_cells )
  implicit none

  class(partition_type), intent(in) :: self

  integer :: num_cells
  integer :: depth   ! loop counter over halo depths

  num_cells = self%num_core + self%num_owned 
  
  do depth = 1,self%halo_depth
    num_cells = num_cells + self%num_halo(depth)
  end do

end function get_num_cells_in_layer


function get_num_cells_core( self ) result ( core_cells )
  implicit none

  class(partition_type), intent(in) :: self

  integer :: core_cells

  core_cells = self%num_core

end function get_num_cells_core


function get_num_cells_owned( self ) result ( owned_cells )
  implicit none

  class(partition_type), intent(in) :: self

  integer :: owned_cells

  owned_cells = self%num_owned

end function get_num_cells_owned


function get_num_cells_halo( self, depth ) result ( halo_cells )
  implicit none

  class(partition_type), intent(in) :: self

  integer, intent(in) :: depth
  integer             :: halo_cells

  halo_cells = self%num_halo(depth)

end function get_num_cells_halo


function get_gid_from_lid( self, lid ) result ( gid )

  implicit none

  class(partition_type), intent(in) :: self

  integer, intent(in) :: lid           ! local index
  integer             :: gid           ! global index
  integer             :: nlayer        ! layer of supplied lid
  integer             :: lid_in_layer  ! supplied lid projected to bottom layer
  integer             :: num_in_list   ! total number of cells in partition
  integer             :: depth         ! loop counter over halo depths

  num_in_list = self%num_core+self%num_owned
  do depth = 1,self%halo_depth
    num_in_list = num_in_list + self%num_halo(depth)
  end do
  lid_in_layer = modulo(lid-1,(num_in_list))+1
  nlayer = (lid-1)/(num_in_list)

  gid = self%global_cell_id(lid_in_layer) + nlayer*(self%global_num_cells)

end function get_gid_from_lid


function get_lid_from_gid( self, gid ) result ( lid )
!
! Perform a search through the global cell lookup table looking for the
! required global index.
!
! The partitioned_cells array holds global indices in three groups: core cells,
! followed by owned cells and finally, the indices of the halo cells. The
! cells are numerically ordered within the different groups so a binary search
! can be used, but not between groups, so need to do separate binary searches
! through the core, owned and halo cells and exit if a match is found
!
  implicit none

  class(partition_type), intent(in) :: self

  integer, intent(in) :: gid           ! global index
  integer             :: lid           ! local index
  integer             :: nlayer        ! layer of supplied gid
  integer             :: gid_in_layer  ! supplied gid projected to bottom layer
  integer             :: depth         ! loop counter over halo depths
  integer             :: start_search  ! start point for a search
  integer             :: end_search    ! end point for a search
  integer             :: num_in_list   ! total number of cells in partition

  num_in_list = self%num_core + self%num_owned
  do depth = 1,self%halo_depth
    num_in_list = num_in_list + self%num_halo(depth)
  end do

  ! Set the default return code
  lid = -1
  ! If the supplied gid is not valid just return
  if(gid < 1) return

  ! The global index lookup table (partitioned_cells) only has the indices for
  ! a single layer, so convert the full 3d global index into the global index
  ! within the layer and a layer number
  gid_in_layer = modulo(gid-1,self%global_num_cells) + 1
  nlayer = (gid-1) / self%global_num_cells

  ! Search though core cells - looking for the gid
  start_search = 1
  end_search = self%num_core
  lid = binary_search( self%global_cell_id( start_search:end_search ), gid )
  if(lid /= -1)then
    lid = lid + nlayer*(num_in_list)  !convert back to 3d lid
    return
  end if

  ! Search though owned cells - looking for the gid
  start_search = end_search + 1
  end_search = start_search + self%num_owned - 1
  lid = binary_search( self%global_cell_id( start_search:end_search ), gid )
  if(lid /= -1)then
    lid = lid + self%num_core + nlayer*(num_in_list)  !convert back to 3d lid
    return
  end if

  ! Search though halo cells - looking for the gid
  do depth = 1,self%halo_depth
    start_search = end_search + 1
    end_search = start_search + self%num_halo(depth) - 1
    lid = binary_search( self%global_cell_id( start_search:end_search ), gid )
    if(lid /= -1)then
      lid = lid + self%num_core + self%num_owned + nlayer*(num_in_list)  !convert back to 3d lid
      return
    end if
  end do

  ! No lid has been found in either the core, owned or halo cells on this partition, so return with lid=-1
  return
  
end function get_lid_from_gid

!> @brief Performs a binary search through the given array looking for
!> a particular entry and returns the index of the entry found
!> or -1 if no matching entry can be found. The values held in "array_to_be_searched"
!> must be in numerically increasing order
!> @param[in] array_to_be_searched The array that will be searched for the given entry
!> @param[in] value_to_find The entry that is to be searched for
pure function binary_search( array_to_be_searched, value_to_find ) result ( id )

  implicit none

  integer, intent(in) :: array_to_be_searched( : )
  integer, intent(in) :: value_to_find
  integer             :: bot, top  ! Lower and upper index limits between which to search for the value
  integer             :: id        ! Next index for refining the search. If an entry is found this will
                                   ! contain the index of the matching entry

  ! Set bot and top to be the whole array to begin with
  bot = 1
  top = size(array_to_be_searched)

  search: do
    ! If top is lower than bot then there is no more array to be searched
    if(top < bot) exit search
    ! Refine the search
    id = (bot+top)/2
    if(array_to_be_searched(id) == value_to_find)then  ! found matching entry
      return
    else if(array_to_be_searched(id) < value_to_find)then ! entry has to be between id and top
      bot = id + 1
    else ! entry has to be between bot and id
      top = id - 1
    endif
  end do search

  ! Didn't find a match - return failure code
  id = -1

end function binary_search

end module partition_mod
